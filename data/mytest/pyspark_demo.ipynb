{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSIÓN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Definición del esquema para los datos\n",
    "customSchema = StructType([\n",
    "    StructField(\"clean_text\", StringType()), \n",
    "    StructField(\"category\", StringType())])\n",
    "\n",
    "# Nombres de los archivos CSV que contienen los datos de tweets\n",
    "filename1 = './Twitter_Data.csv'\n",
    "filename2 = './Twitter_Data.csv'\n",
    "\n",
    "# Lectura de datos desde los archivos CSV\n",
    "df1 = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename1)\n",
    "df2 = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename2)\n",
    "\n",
    "# Unión de los datos de los dos archivos\n",
    "df = df1.union(df2)\n",
    "\n",
    "# Eliminación de filas con valores nulos\n",
    "data = df.na.drop(how='any')\n",
    "\n",
    "# Agrupación de los datos por la columna \"category\" y conteo de las categorías\n",
    "data.groupBy(\"category\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Configuración de transformaciones para procesar el texto\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Tokenización del texto utilizando una expresión regular\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Definición de palabras vacías (stop words)\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Creación de una representación de \"bag of words\" a partir de las palabras tokenizadas\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Configuración de una canalización (pipeline) para aplicar las transformaciones\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Conversión de la columna \"category\" a etiquetas numéricas\n",
    "label_stringIdx = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# Definición de la canalización\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Ajuste de la canalización a los datos\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "# Entrenamiento de un modelo de regresión logística\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Realización de predicciones en el conjunto de prueba\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Mostrar las 10 primeras predicciones ordenadas por probabilidad\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\", \"category\", \"probability\", \"label\", \"prediction\")\\\n",
    "    .orderBy(\"probability\", ascending=False).show(n=10, truncate=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSIÓN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crear una instancia de SparkSession\n",
    "spark = SparkSession.builder.appName(\"NombreDeLaApp\").getOrCreate()\n",
    "\n",
    "# Definición del esquema para los datos\n",
    "customSchema = StructType([\n",
    "    StructField(\"clean_text\", StringType()), \n",
    "    StructField(\"category\", StringType())])\n",
    "\n",
    "# Nombres de los archivos CSV que contienen los datos de tweets\n",
    "filename1 = './Twitter_Data.csv'\n",
    "filename2 = './Twitter_Data.csv'\n",
    "\n",
    "# Lectura de datos desde los archivos CSV\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename1)\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename2)\n",
    "\n",
    "# Unión de los datos de los dos archivos\n",
    "df = df1.union(df2)\n",
    "\n",
    "# Eliminación de filas con valores nulos\n",
    "data = df.na.drop(how='any')\n",
    "\n",
    "# Agrupación de los datos por la columna \"category\" y conteo de las categorías\n",
    "data.groupBy(\"category\").agg(F.count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
    "\n",
    "# Configuración de transformaciones para procesar el texto\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Tokenización del texto utilizando una expresión regular\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Definición de palabras vacías (stop words)\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Creación de una representación de \"bag of words\" a partir de las palabras tokenizadas\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Configuración de una canalización (pipeline) para aplicar las transformaciones\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Conversión de la columna \"category\" a etiquetas numéricas\n",
    "label_stringIdx = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# Definición de la canalización\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Ajuste de la canalización a los datos\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "# Entrenamiento de un modelo de regresión logística\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Realización de predicciones en el conjunto de prueba\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Mostrar las 10 primeras predicciones ordenadas por probabilidad\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\", \"category\", \"probability\", \"label\", \"prediction\")\\\n",
    "    .orderBy(\"probability\", ascending=False).show(n=10, truncate=30)\n",
    "\n",
    "# Detener la sesión de pyspark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSIÓN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Crear una instancia de SparkSession\n",
    "spark = SparkSession.builder.appName(\"SocialApp\").getOrCreate()\n",
    "\n",
    "# Definición del esquema para los datos\n",
    "customSchema = StructType([\n",
    "    StructField(\"clean_text\", StringType()), \n",
    "    StructField(\"category\", StringType())])\n",
    "\n",
    "# Nombres de los archivos CSV que contienen los datos de tweets\n",
    "filename1 = './Twitter_Data.csv'\n",
    "filename2 = './Twitter_Data.csv'\n",
    "\n",
    "# Lectura de datos desde los archivos CSV\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename1)\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename2)\n",
    "\n",
    "# Unión de los datos de los dos archivos\n",
    "df = df1.union(df2)\n",
    "\n",
    "# Eliminación de filas con valores nulos\n",
    "data = df.na.drop(how='any')\n",
    "\n",
    "# Agrupación de los datos por la columna \"category\" y conteo de las categorías\n",
    "data.groupBy(\"category\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Configuración de transformaciones para procesar el texto\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Tokenización del texto utilizando una expresión regular\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Definición de palabras vacías (stop words)\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Creación de una representación de \"bag of words\" a partir de las palabras tokenizadas\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Configuración de una canalización (pipeline) para aplicar las transformaciones\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Conversión de la columna \"category\" a etiquetas numéricas\n",
    "label_stringIdx = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# Definición de la canalización\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Ajuste de la canalización a los datos\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "# Entrenamiento de un modelo de regresión logística\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Realización de predicciones en el conjunto de prueba\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Mostrar las 10 primeras predicciones ordenadas por probabilidad\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\", \"category\", \"probability\", \"label\", \"prediction\")\\\n",
    "    .orderBy(\"probability\", ascending=False).show(n=10, truncate=30)\n",
    "\n",
    "# Detener la sesión de pyspark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSIÓN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Crear una instancia de SparkSession\n",
    "spark = SparkSession.builder.appName(\"SocialApp\").getOrCreate()\n",
    "\n",
    "# Definición del esquema para los datos\n",
    "customSchema = StructType([\n",
    "    StructField(\"clean_text\", StringType()), \n",
    "    StructField(\"category\", StringType())])\n",
    "\n",
    "# Nombres de los archivos CSV que contienen los datos de tweets\n",
    "filename = './Twitter_Data.csv'\n",
    "\n",
    "# Lectura de datos desde los archivos CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename)\n",
    "\n",
    "# Eliminación de filas con valores nulos\n",
    "data = df.na.drop(how='any')\n",
    "\n",
    "# Agrupación de los datos por la columna \"category\" y conteo de las categorías\n",
    "data.groupBy(\"category\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Configuración de transformaciones para procesar el texto\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Tokenización del texto utilizando una expresión regular\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Definición de palabras vacías (stop words)\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Creación de una representación de \"bag of words\" a partir de las palabras tokenizadas\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Configuración de una canalización (pipeline) para aplicar las transformaciones\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Conversión de la columna \"category\" a etiquetas numéricas\n",
    "label_stringIdx = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# Definición de la canalización\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Ajuste de la canalización a los datos\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "# Entrenamiento de un modelo de regresión logística\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Realización de predicciones en el conjunto de prueba\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Mostrar las 10 primeras predicciones ordenadas por probabilidad\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\", \"category\", \"probability\", \"label\", \"prediction\")\\\n",
    "    .orderBy(\"probability\", ascending=False).show(n=10, truncate=30)\n",
    "\n",
    "# Detener la sesión de pyspark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSIÓN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                    clean_text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|vote for pappu straight mig...|       1|[0.9992425267693166,5.37945...|  0.0|       0.0|\n",
      "|appeasement hai mumkeen hai...|       1|[0.9988912543452395,3.85556...|  0.0|       0.0|\n",
      "|its always great blessing f...|       1|[0.9986654466344741,1.93836...|  0.0|       0.0|\n",
      "|respected sir sar and madam...|       1|[0.998501564523588,9.190434...|  0.0|       0.0|\n",
      "|absolutelyyoure talented ac...|       1|[0.9983340032234882,4.59319...|  0.0|       0.0|\n",
      "|respected sir sar and madam...|       1|[0.9983128327480091,9.81409...|  0.0|       0.0|\n",
      "|respected shree narendrabha...|       1|[0.9980153739121862,0.00101...|  0.0|       0.0|\n",
      "|leaders and their promises ...|       1|[0.9976862987065443,8.01212...|  0.0|       0.0|\n",
      "|dear dad retired general gr...|       1|[0.9976253229032603,4.71787...|  0.0|       0.0|\n",
      "|respected sir sar and madam...|       1|[0.9975754236566232,0.00125...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Crear una instancia de SparkSession\n",
    "spark = SparkSession.builder.appName(\"SocialApp\").getOrCreate()\n",
    "\n",
    "# Definición del esquema para los datos\n",
    "customSchema = StructType([\n",
    "    StructField(\"clean_text\", StringType()), \n",
    "    StructField(\"category\", StringType())])\n",
    "\n",
    "# Nombres de los archivos CSV que contienen los datos de tweets\n",
    "filename = './Twitter_Data.csv'\n",
    "\n",
    "# Lectura de datos desde los archivos CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename)\n",
    "\n",
    "# Eliminación de filas con valores nulos\n",
    "data = df.na.drop(how='any')\n",
    "\n",
    "# Agrupación de los datos por la columna \"category\" y conteo de las categorías\n",
    "data.groupBy(\"category\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Tokenización del texto utilizando una expresión regular\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Definición de palabras vacías (stop words)\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Creación de una representación de \"bag of words\" a partir de las palabras tokenizadas\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Conversión de la columna \"category\" a etiquetas numéricas\n",
    "label_stringIdx = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# Definición de la canalización\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Ajuste de la canalización a los datos\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "# Entrenamiento de un modelo de regresión logística\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Realización de predicciones en el conjunto de prueba\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Mostrar las 10 primeras predicciones ordenadas por probabilidad\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\", \"category\", \"probability\", \"label\", \"prediction\")\\\n",
    "    .orderBy(\"probability\", ascending=False).show(n=10, truncate=30)\n",
    "\n",
    "# Detener la sesión de pyspark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
