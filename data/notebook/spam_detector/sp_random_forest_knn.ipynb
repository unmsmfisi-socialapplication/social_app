{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import requests\n",
    "from io import StringIO\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Libraries to handle data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Libraries for natural language processing re (regular expressions) and nltk (NLP)\n",
    "import re\n",
    "import nltk\n",
    "# Tools for NLP like stopwords, stemmer, vectorizer, encoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Model Training Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Modelos de Random Forest y KNN (Clasificadores)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Tools for evaluation\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team considers it necessary to mention that a generic data set \"spam.csv\" is used as a model test, which includes paragraphs or sentences with a variety of words (size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/spam.csv\", encoding = \"ISO-8859-1\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"]\n",
    "data = data.drop(data[to_drop], axis=1)\n",
    "# Rename Columns\n",
    "data.rename(columns = {\"v1\":\"Target\", \"v2\":\"Text\"}, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section represents the exploration of features and creation of new assets in the data like:\n",
    "* No_of_Characters: Number of characters\n",
    "* No_of_Words: Number of words\n",
    "* No_of_sentence: Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dowload of Punk Sentence Tokenizer\n",
    "# Tokenizer divides text in sentences\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding new columns (features)\n",
    "data[\"No_of_Characters\"] = data[\"Text\"].apply(len)\n",
    "data[\"No_of_Words\"] = data.apply(\n",
    "    lambda row: nltk.word_tokenize(row[\"Text\"]), axis=1).apply(len) #Tokenization of Text words\n",
    "data[\"No_of_sentence\"] = data.apply(\n",
    "    lambda row: nltk.sent_tokenize(row[\"Text\"]), axis=1).apply(len) #Tokenization of  Text sentences\n",
    "\n",
    "data.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even as just a test , the data could contain non-typical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "cols= [\"#E1F16B\", \"#E598D8\"]\n",
    "fg = sns.pairplot(data=data, hue=\"Target\", palette=cols)\n",
    "plt.show(fg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the number of characters (due to so much non-typical values) to less than 350 per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminando los valores at√≠picos\n",
    "data = data[(data[\"No_of_Characters\"] < 350)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cleansing process in Natural Language Processing (NLP) is crucial. Without it, the computer does not understand the text and interprets it as a meaningless set of symbols. For more complex data processing we require data cleansing compulsorily.\n",
    "\n",
    "* First stage: Extract alphabetical characters then punctuations and numbers\n",
    "* Second stage: Lowercase characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean function definition\n",
    "def Clean(Text):\n",
    "    sms = re.sub('[^a-zA-Z]', ' ', Text) #The regular expression of [^a-zA-Z] replaces non alphabetical into blanks\n",
    "    sms = sms.lower() #Lowercase convertion\n",
    "    sms = sms.split()\n",
    "    sms = ' '.join(sms)\n",
    "    return sms\n",
    "\n",
    "#New column for clean text\n",
    "data[\"Clean_Text\"] = data[\"Text\"].apply(Clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is about spliting complex data in little units called tokens . In this case , pharagraphs into words and sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying lambda function for tokenization\n",
    "data[\"Tokenize_Text\"]=data.apply(lambda row: nltk.word_tokenize(row[\"Clean_Text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are words with high frecuency but with no special contribution to NLP (like , no , few) . NLTK library has default stopwords for elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords delete function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "data[\"Nostopword_Text\"] = data[\"Tokenize_Text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is about obtaining the core of the words , but ensuring the correct translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the lemmatizer parameter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for lemmatize a word\n",
    "def lemmatize_word(text):\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas\n",
    "\n",
    "#New column with lemmatize text\n",
    "data[\"Lemmatized_Text\"] = data[\"Nostopword_Text\"].apply(lemmatize_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF in NLP stands for Term Frequency: Inverse Document Frequency. In NLP, clean data must be converted to a numeric format where each word is represented by a matrix. This is also known as word embedding or word vectorization.\n",
    "\n",
    "Term Frequency (TF) = (Frequency of a term in the document)/(Total number of terms in the documents) Inverse Document Frequency (IDF) = log ((Total number of documents)/(Number of documents with the term t)) I'll use TfidfVectorizer() to vectorize the preprocessed data.\n",
    "\n",
    "Steps in Vectorization:\n",
    "\n",
    "* Creation of a lemmatized text corpus.\n",
    "* Convert the corpus to vector form\n",
    "* Label encoding of classes in Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create corpus (More vectorized text)\n",
    "corpus = []\n",
    "for i in data[\"Lemmatized_Text\"]:\n",
    "    msg = ' '.join([row for row in i])\n",
    "    corpus.append(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text into numbers by Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "#Creating an X data section (Independent variables)\n",
    "X = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Y variable , the target of spam or non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data[\"Target\"] = label_encoder.fit_transform(data[\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruction of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Y , the target and sets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Target\"] \n",
    "# Train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300,max_depth=10,min_samples_split=5,min_samples_leaf=2,max_features='auto',random_state=42)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5,weights='uniform',n_jobs=-1,algorithm='auto')\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating metrics for both models\n",
    "precision =[]\n",
    "recall =[]\n",
    "f1_score = []\n",
    "\n",
    "#List of models\n",
    "classifiers = [rf,knn]\n",
    "\n",
    "# Evaluating each model\n",
    "for i in classifiers:\n",
    "    pred_train = i.predict(X_train)\n",
    "    pred_test = i.predict(X_test)\n",
    "    prec = metrics.precision_score(y_test, pred_test)\n",
    "    recal = metrics.recall_score(y_test, pred_test)\n",
    "    f1_s = metrics.f1_score(y_test, pred_test)\n",
    "\n",
    "  \n",
    "    # Results\n",
    "    precision.append(prec)\n",
    "    recall.append(recal)\n",
    "    f1_score.append(f1_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the testing we shall initiate a list of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {'Precision':precision,\n",
    "'Recall':recall,\n",
    "'F1score':f1_score}\n",
    "# Creando dataframe en pandas\n",
    "Results = pd.DataFrame(evaluation, index =[ \"RandomForest\", \"KNeighbours\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results dataframe is shown in colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap2 = ListedColormap([\"#E2CCFF\", \"#E598D8\"])\n",
    "Results.style.background_gradient(cmap=cmap2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally , a graphic of the evaluation for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ListedColormap([\"#E1F16B\", \"#E598D8\"])\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "\n",
    "for cls, ax in zip(classifiers, axes.flatten()):\n",
    "    plot_confusion_matrix(cls,\n",
    "                          X_test,\n",
    "                          y_test,\n",
    "                          ax=ax,\n",
    "                          cmap=cmap,\n",
    "                          )\n",
    "    ax.title.set_text(type(cls).__name__)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Conclusions of testing are documented in the respective data issue "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
