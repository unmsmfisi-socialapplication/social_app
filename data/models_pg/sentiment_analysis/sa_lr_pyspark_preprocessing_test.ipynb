{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TEST] **sa_lr_pyspark_preprocessing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test set focuses on functions related to data preprocessing in the sa_lr_pyspark_preprocessing module.  \n",
    "\n",
    "The primary purpose of these tests is to verify that data cleaning and transformation operations are performed correctly using the functions provided in this module.  \n",
    "\n",
    "Mocks are used to simulate Spark operations and ensure they are called correctly.  \n",
    "\n",
    "Additionally, these tests seek to confirm that the resulting DataFrame is valid and not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from mock import Mock\n",
    "\n",
    "from preprocessing.sa_lr_pyspark_preprocessing import pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def spark_session():\n",
    "    # Set up a SparkSession for testing\n",
    "    return SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pre_process(spark_session):\n",
    "    # Create a test DataFrame\n",
    "    data = [\n",
    "        (1, \"This is a sample tweet.\"),\n",
    "        (0, \"Another tweet with numbers 123.\"),\n",
    "    ]\n",
    "    columns = [\"label\", \"text\"]\n",
    "    df = spark_session.createDataFrame(data, columns)\n",
    "\n",
    "    # Mock for the Spark functions used in pre_process\n",
    "    MockTokenizer = Mock()\n",
    "    MockStopWordsRemover = Mock()\n",
    "    MockCountVectorizer = Mock()\n",
    "    MockIDF = Mock()\n",
    "\n",
    "    # Replace the Spark functions with the mocks\n",
    "    pre_process.tokenizer = MockTokenizer\n",
    "    pre_process.remover = MockStopWordsRemover\n",
    "    pre_process.count = MockCountVectorizer\n",
    "    pre_process.idf = MockIDF\n",
    "\n",
    "    # Call the pre_process function with the test DataFrame\n",
    "    result = pre_process(df)\n",
    "\n",
    "    # Verify that the Spark functions were called correctly\n",
    "    MockTokenizer.assert_called_once_with(inputCol=\"text\", outputCol=\"words\")\n",
    "    MockStopWordsRemover.assert_called_once_with(inputCol=\"words\", outputCol=\"word_clean\")\n",
    "    MockCountVectorizer.assert_called_once_with(inputCol=\"word_clean\", outputCol=\"rawFeatures\")\n",
    "    MockIDF.assert_called_once_with(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "    # Verify that the result is not None\n",
    "    assert result is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pytest.main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
