{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the libraries and classes needed to perform data processing and text classification tasks using PySpark and other related libraries are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark as ps\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import warnings\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to initialize a Spark session in local mode with certain configurations (4 threads on my PC) and returns the Spark session so that it can be used in the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    print(\"Initializing Spark...\")\n",
    "    try:\n",
    "        sc = ps.SparkContext('local[4]')\n",
    "        sqlContext = SQLContext(sc)\n",
    "        print(\"Just created a SparkContext\")\n",
    "    except ValueError:\n",
    "        warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to read a CSV file into Spark and load it into a DataFrame. The resulting DataFrame can then be used to perform data processing and analysis operations in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fileUrl, spark):\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = spark.read.csv(fileUrl, sep=\",\", inferSchema=True, header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs a series of text preprocessing operations on a PySpark DataFrame, including character cleansing, tokenization, stopword removal, stemming, and numeric feature extraction, thereby preparing the data for further analysis or machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    print(\"Preprocessing...\")\n",
    "    df.count()\n",
    "    \n",
    "    df = df.withColumnRenamed('_c0', \"id\").withColumnRenamed('_c1', 'label').withColumnRenamed('_c2', 'tweet')\n",
    "    \n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', '[^a-z0-9A-Z`~!@#$%&<>?., ]', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', '[0-9`~!@#$%&<>?,\\']', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', 'http://*.*.com', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', 'www.*.com', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', '\\.', ''))\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "    wordData = tokenizer.transform(df)\n",
    "    \n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"word_clean\")\n",
    "    word_clean_data = remover.transform(wordData)\n",
    "    \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    \n",
    "    count = CountVectorizer(inputCol=\"word_clean\", outputCol=\"rawFeatures\")\n",
    "    model = count.fit(word_clean_data)\n",
    "    \n",
    "    featurizedData = model.transform(word_clean_data)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    \n",
    "    return rescaledData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to split a PySpark DataFrame into training and test sets, which is a common operation in machine learning to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    print(\"Splitting dataset...\")\n",
    "    seed = 0\n",
    "    trainDf, testDf = df.randomSplit([0.7, 0.3], seed)\n",
    "    trainDf.count()\n",
    "    testDf.count()\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes the predictions from the training set and the test set, and displays detailed tables indicating how many instances were correctly predicted (label match) and how many were incorrectly predicted (difference between the actual label and the predicted label) for each. set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_table(train_predictions, test_predictions):\n",
    "    print(\"Training Table...\")\n",
    "    train_predictions.groupBy('label','prediction').count().show()\n",
    "\n",
    "    print(\"Test Table...\")\n",
    "    test_predictions.groupBy('label','prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to calculate and summarize various common evaluation metrics for a classification model from a set of predictions.\n",
    "Metrics included are area under the ROC curve, F1 metric, and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, labelCol=\"label\", predictionCol=\"prediction\"):\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=predictionCol, labelCol=labelCol, metricName=\"areaUnderROC\")\n",
    "    roc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return {\"ROC\": roc, \"F1\": f1, \"Accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements the training and evaluation of a Naive Bayes classification model with hyperparameter search using cross-validation in Spark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_Bayes(train_data, test_data):\n",
    "    print(\"Using Naive Bayes model with train and test data...\")\n",
    "    \n",
    "    nb = NaiveBayes()\n",
    "    paramGrid_nb = ParamGridBuilder() \\\n",
    "        .addGrid(nb.smoothing, np.linspace(0.3, 10, 10)) \\\n",
    "        .build()\n",
    "    \n",
    "    crossval_nb = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid_nb, evaluator=BinaryClassificationEvaluator(), numFolds=5)\n",
    "    cvModel_nb = crossval_nb.fit(train_data)\n",
    "    \n",
    "    train_predictions = cvModel_nb.transform(train_data)\n",
    "    train_summary = evaluate_model(train_predictions)\n",
    "    \n",
    "    test_predictions = cvModel_nb.transform(test_data)\n",
    "    test_summary = evaluate_model(test_predictions)\n",
    "\n",
    "    details_table(train_predictions, test_predictions)\n",
    "    \n",
    "    return train_summary, test_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell initializes a Spark session, which is required to work with Spark and run distributed data processing tasks in a cluster.\n",
    "A CSV file is then uploaded, performs preprocessing on the data, and then splits the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "\n",
    "url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSoAtoF35YFTr-hDjec2LdghHmryR7nSpmolXIuhNh-CCTTY9oCQq4gUlBLCu86oWslt6nqV_Z1jGcJ/pub?output=csv\"\n",
    "\n",
    "df = read_file(url, spark)\n",
    "df = pre_process(df)\n",
    "\n",
    "train_data, test_data = train_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a series of training experiments and evaluation of machine learning models are carried out using the algorithms Naive Bayes in the training and test sets.\n",
    "Training and test summaries are printed for each algorithm and data set, allowing you to assess the performance of your models on different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary, test_summary = naive_Bayes(train_data, test_data)\n",
    "\n",
    "print(\"Naive Bayes Train Summary:\")\n",
    "print(train_summary)\n",
    "print(\"\")\n",
    "print(\"Naive Bayes Test Summary:\")\n",
    "print(test_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
