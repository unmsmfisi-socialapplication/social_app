{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TEST] sa_lr_pyspark_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test set covers the complete workflow of the sa_lr_pyspark_main module.  \n",
    "\n",
    "It verifies functions for file reading, data preprocessing, splitting into training and test sets, and model training.  \n",
    "\n",
    "The main purpose is to confirm that the entire workflow functions smoothly and that functions are called and executed correctly.  \n",
    "\n",
    "Mocks are used to simulate Spark operations in the read_file function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from mock import Mock\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "from execution.sa_lr_pyspark_main import read_file, logistic_regression_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def spark_session():\n",
    "    # Set up a SparkSession for testing\n",
    "    return SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_read_file(spark_session):\n",
    "    # Create a test DataFrame\n",
    "    data = [\n",
    "        (1, \"This is a sample tweet.\"),\n",
    "        (0, \"Another tweet with numbers 123.\"),\n",
    "    ]\n",
    "    columns = [\"label\", \"text\"]\n",
    "    df = spark_session.createDataFrame(data, columns)\n",
    "\n",
    "    # Mock for the SparkSession.read.csv function\n",
    "    spark_session.read.csv = Mock(return_value=df)\n",
    "\n",
    "    # Call the read_file function with a fake file\n",
    "    fileUrl = \"fake_file.csv\"\n",
    "    result = read_file(fileUrl, spark_session)\n",
    "\n",
    "    # Verify that the SparkSession.read.csv function was called correctly\n",
    "    spark_session.read.csv.assert_called_once_with(fileUrl, sep=\",\", inferSchema=True, header=False)\n",
    "\n",
    "    # Verify that the result is a DataFrame\n",
    "    assert isinstance(result, DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression_workflow(spark_session):\n",
    "    # Create a test DataFrame\n",
    "    data = [\n",
    "        (1, \"This is a sample tweet.\"),\n",
    "        (0, \"Another tweet with numbers 123.\"),\n",
    "    ]\n",
    "    columns = [\"label\", \"text\"]\n",
    "    df = spark_session.createDataFrame(data, columns)\n",
    "\n",
    "    # Mocks for the functions used in the workflow\n",
    "    pre_process_mock = Mock(return_value=df)\n",
    "    train_test_split_mock = Mock(return_value=(df, df))\n",
    "    logistic_regression_mock = Mock(return_value=(\"train_summary\", \"test_summary\"))\n",
    "\n",
    "    # Call the complete workflow\n",
    "    train_summary, test_summary = logistic_regression_workflow(df, pre_process_mock, train_test_split_mock, logistic_regression_mock)\n",
    "\n",
    "    # Verify that all functions were called correctly\n",
    "    pre_process_mock.assert_called_once_with(df)\n",
    "    train_test_split_mock.assert_called_once_with(df)\n",
    "    logistic_regression_mock.assert_called_once_with(df, df)\n",
    "\n",
    "    # Verify the results\n",
    "    assert train_summary == \"train_summary\"\n",
    "    assert test_summary == \"test_summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_workflow(data, pre_process_function, train_test_split_function, logistic_regression_function):\n",
    "    df = pre_process_function(data)\n",
    "    train_data, test_data = train_test_split_function(df)\n",
    "    train_summary, test_summary = logistic_regression_function(train_data, test_data)\n",
    "    return train_summary, test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pytest.main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
