{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NiSqRWBYipD"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Si tenemos una GPU disponible, configuraremos nuestro dispositivo en GPU. Usaremos esta variable de dispositivo más adelante en nuestro código.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU esta disponible\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU no dispoible, CPU usada\")"
      ],
      "metadata": {
        "id": "aCTcSjrl4hpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_csv = '/content/sample_data/IMDB Dataset.csv'\n",
        "df = pd.read_csv(base_csv, delimiter=\",\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IKFb5Cw2ZKWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### División para entrenar y probar datos"
      ],
      "metadata": {
        "id": "Fum9snvMeo1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividiremos los datos para entrenar y probar inicialmente. Hacer esto en una etapa anterior permite evitar la fuga de datos."
      ],
      "metadata": {
        "id": "zmWGasVpe9KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = df['review'].values,df['sentiment'].values\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')"
      ],
      "metadata": {
        "id": "8ciWUmuJfDAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analizando los sentimientos"
      ],
      "metadata": {
        "id": "1as65c4yjYxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dd = pd.Series(y_train).value_counts()\n",
        "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lmNMLgPaf6Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tockenización"
      ],
      "metadata": {
        "id": "fxgmbrAuj1tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_string(s):\n",
        "    # Eliminar todos los caracteres que no sean palabras (todo excepto números y letras)\n",
        "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Reemplace todas las tiradas de espacios en blanco sin espacio\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    # reemplazar dígitos sin espacio\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def tockenize(x_train,y_train,x_val,y_val):\n",
        "    word_list = []\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_string(word)\n",
        "            if word not in stop_words and word != '':\n",
        "                word_list.append(word)\n",
        "\n",
        "    corpus = Counter(word_list)\n",
        "    # ordenar según las palabras más comunes\n",
        "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
        "    # creando un dictado\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "\n",
        "    # tokenizar\n",
        "    final_list_train,final_list_test = [],[]\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
        "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
        "    for sent in x_val:\n",
        "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
        "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
        "\n",
        "    encoded_train = [1 if label =='positive' else 0 for label in y_train]\n",
        "    encoded_test = [1 if label =='positive' else 0 for label in y_val]\n",
        "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
      ],
      "metadata": {
        "id": "lcau4C_7jjC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "l0f0pkKSm7Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'La longitud del vocabulario es {len(vocab)}')"
      ],
      "metadata": {
        "id": "GGLxPLv9nHEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analizando la longitud de los reviews"
      ],
      "metadata": {
        "id": "_0vy-kB0pS6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev_len = [len(i) for i in x_train]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "pd.Series(rev_len).describe()"
      ],
      "metadata": {
        "id": "grIiVMxVpNsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rellenando datos"
      ],
      "metadata": {
        "id": "jvo2a0orplTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora rellenaremos cada una de las secuencias hasta alcanzar la longitud máxima."
      ],
      "metadata": {
        "id": "r7T5Vt2Epngp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ],
      "metadata": {
        "id": "gvvmGlhOpdBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tenemos muy pocas reseñas con una extensión mayor a 500 caracteres.\n",
        "#Así que consideraremos sólo aquellos que tienen menos de esa cantidad de caracteres\n",
        "x_train_pad = padding_(x_train,500)\n",
        "x_test_pad = padding_(x_test,500)"
      ],
      "metadata": {
        "id": "hbDbAkcUwB2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Procesamiento por lotes y cargando como tensor"
      ],
      "metadata": {
        "id": "yRuWt2EW0Mzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creando conjuntos de datos tensoriales\n",
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
        "\n",
        "# cargadores de datos\n",
        "batch_size = 50\n",
        "\n",
        "# mezclando los datos\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "j7feSoofwdtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obteneniendo un lote de datos de entrenamiento\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "\n",
        "print('Tamaño de muestra de entrada: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Muestra de entrada: \\n', sample_x)\n",
        "print('Muestra de entrada: \\n', sample_y)"
      ],
      "metadata": {
        "id": "V94eTeiU01fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MODELO"
      ],
      "metadata": {
        "id": "DpvIM0b22hMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # embeddings y capas LSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "\n",
        "\n",
        "        # capa de dropout\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # capa lineal y sigmoidea\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings y lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        #print(embeds.shape)  #[50, 500, 1000]\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # dropout y capa completamente conectada\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # función sigmoidea\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        # remodelar para que sea batch_size primero\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        sig_out = sig_out[:, -1] # obtener el último lote de etiquetas\n",
        "\n",
        "        # devolver la última salida sigmoidea y el estado oculto\n",
        "        return sig_out, hidden\n",
        "\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Crea dos nuevos tensores con tamaños n_layers x batch_size x hidden_dim,\n",
        "        # inicializado a cero, para estado oculto y estado de celda de LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "_0-jDKh32AtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 #aumentado en 1 para rellenar\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "\n",
        "\n",
        "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
        "\n",
        "#Moviendo al gpu\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "yZVLFNLs3-P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1E3xQ3HV4yqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}