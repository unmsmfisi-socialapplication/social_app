{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "def4c010"
      },
      "source": [
        "# ANALYSIS OF USER SENTIMENT ON TWITTER IN SPANISH ABOUT TECHNOLOGIES APPLYING TEXT MINING"
      ],
      "id": "def4c010"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b77fe9ce"
      },
      "source": [
        "# INSTALLATION OF LIBRARIES WITH PIP INSTALL"
      ],
      "id": "b77fe9ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75114d60"
      },
      "outputs": [],
      "source": [
        "# Installing the Twint library from GitHub\n",
        "#!pip install --user --upgrade -e git+https://github.com/twintproject/twint.git#egg=twint\n",
        "!pip install --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "#!pip uninstall twint\n",
        "#!pip install git+git://github.com/ajctrl/twint@patch-1\n"
      ],
      "id": "75114d60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ebda7c"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ],
      "id": "b9ebda7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad45211e"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ],
      "id": "ad45211e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "477c3859"
      },
      "outputs": [],
      "source": [
        "# Installation of the Pipeline library\n",
        "!pip install pipeline"
      ],
      "id": "477c3859"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b03b7b8"
      },
      "outputs": [],
      "source": [
        "# Installing the Transformers library\n",
        "!pip install transformers"
      ],
      "id": "5b03b7b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781a76db"
      },
      "source": [
        "# IMPORTAR LIBRER√çAS"
      ],
      "id": "781a76db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f85509a"
      },
      "outputs": [],
      "source": [
        "#For handling DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "#For matrix management\n",
        "import numpy as np\n",
        "\n",
        "#To draw graphs\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#For handling dates and times\n",
        "from datetime import date, timedelta\n",
        "\n",
        "#For Twitter data extraction\n",
        "import twint\n",
        "\n",
        "#When the Twint library is executed in Jupyter, the following error appears\n",
        "# -> RuntimeError: This event loop is already running\n",
        "#To solve this problem we can insert the anest_asyncio library\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#For natural language processing\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "#To find regular expressions such as characters\n",
        "#special or alphabet\n",
        "import re\n",
        "\n",
        "#To build language processing applications\n",
        "#natural (NLP) lemmatization\n",
        "import spacy\n",
        "\n",
        "#For the analysis of feelings\n",
        "from transformers import pipeline\n",
        "\n",
        "import collections\n",
        "import itertools"
      ],
      "id": "1f85509a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d370a036"
      },
      "source": [
        "# CONFIGURATION AND APPLICATION OF FILTERS IN TWINT FOR SEARCHING ON TWITTER"
      ],
      "id": "d370a036"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a39f21af"
      },
      "outputs": [],
      "source": [
        "# Setting\n",
        "c = twint.Config()\n",
        "\n",
        "c.Search = \"Technology\" #To extract only the tweets that contain this word\n",
        "c.Limit = 100000 #To indicate the maximum number of tweets to collect\n",
        "c.Since = \"2023-01-01\" #To extract the tweets that have been published since the date we consider.\n",
        "c.Pandas = True #To be able to store the results in a Dataframe from the Pandas library\n",
        "#c.Elasticsearch = \"http://localhost:9200\"\n",
        "twint.run.Search(c) #Start searching\n",
        "\n",
        "#Parameters taken from the documentation: https://github.com/twintproject/twint/wiki/Configuration"
      ],
      "id": "a39f21af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0a43246"
      },
      "source": [
        "## Put Tweets in a Dataframe with Pandas"
      ],
      "id": "d0a43246"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57f80f43"
      },
      "outputs": [],
      "source": [
        "# Function to be able to see the columns of the extracted DataFrame\n",
        "def column_names():\n",
        "     return twint.output.panda.Tweets_df.columns\n",
        "#Function to save the columns in a Dataframe\n",
        "def twint_to_pd(columns):\n",
        "     return twint.output.panda.Tweets_df[columns]"
      ],
      "id": "57f80f43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1b3ef86"
      },
      "outputs": [],
      "source": [
        "#see all columns\n",
        "print(column_names())"
      ],
      "id": "b1b3ef86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c56e796"
      },
      "outputs": [],
      "source": [
        "# We store all the data in a DataFrame\n",
        "tweet_df = twint_to_pd(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place',\n",
        "       'tweet', 'language', 'hashtags', 'cashtags', 'user_id', 'user_id_str',\n",
        "       'username', 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
        "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
        "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
        "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
        "       'trans_dest'])"
      ],
      "id": "3c56e796"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e464e13c"
      },
      "outputs": [],
      "source": [
        "#Print the DataFrame\n",
        "tweet_df"
      ],
      "id": "e464e13c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2af14bc"
      },
      "outputs": [],
      "source": [
        "#See the total number of rows in the data lake\n",
        "Tweet df.shape"
      ],
      "id": "a2af14bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "561ebeba"
      },
      "outputs": [],
      "source": [
        "#We save the Data Lake in a csv\n",
        "tweet df.to_csv(\"DataLake.csv\", index=False)"
      ],
      "id": "561ebeba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a3858f2"
      },
      "source": [
        "## Using Required Columns"
      ],
      "id": "9a3858f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dd404d7"
      },
      "outputs": [],
      "source": [
        "# We use specific columns\n",
        "tweet_df = twint_to_pd([\"date\",\"username\",\"language\",\"tweet\",\"nlikes\"])"
      ],
      "id": "0dd404d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a32d8a27"
      },
      "outputs": [],
      "source": [
        "#Print the DataFrame\n",
        "tweet_df"
      ],
      "id": "a32d8a27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7095aa20"
      },
      "outputs": [],
      "source": [
        "#We save only the columns that we are going to use in a csv\n",
        "tweet_df.to_csv(\"DataTwitter.csv\", index=False)"
      ],
      "id": "7095aa20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77c2e623"
      },
      "source": [
        "# WE LIMIT THE DATALAKE TO SPANISH"
      ],
      "id": "77c2e623"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc268837"
      },
      "outputs": [],
      "source": [
        "#We limit the dataframe to Spanish (es)\n",
        "tweet_eng = tweet_df[tweet_df['language']=='en']"
      ],
      "id": "bc268837"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b44216e7"
      },
      "outputs": [],
      "source": [
        "#Print the DataFrame\n",
        "tweet_esp"
      ],
      "id": "b44216e7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44a61800"
      },
      "outputs": [],
      "source": [
        "#We save with the Tweets filter in Spanish\n",
        "tweet_eng.to_csv(\"DataTwitterES.csv\", index=False)"
      ],
      "id": "44a61800"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e3aca9"
      },
      "source": [
        "## EDA of Dates"
      ],
      "id": "06e3aca9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98e98c76"
      },
      "outputs": [],
      "source": [
        "# Convert the \"Date\" column to a list\n",
        "dates_list = tweet_eng['date'].to_list()"
      ],
      "id": "98e98c76"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0701834d"
      },
      "outputs": [],
      "source": [
        "\n",
        "dates = []\n",
        "for t in dates_list:\n",
        "     # extract the date part from the date time\n",
        "     date_str = t.split(' ')[0]\n",
        "     # extract the time from the date\n",
        "     year,month,day = [int(i) for i in date_str.split('-')]\n",
        "     # create a date object\n",
        "     d = date(year, month, day)\n",
        "     #sort\n",
        "     dates.append(d)\n",
        "\n",
        "# sort dates\n",
        "dates.sort()\n",
        "\n",
        "# find the first and last date\n",
        "min_date = dates[0]\n",
        "max_date = dates[-1]\n",
        "\n",
        "# calculate the number of days\n",
        "length = (max_date - min_date).days + 1\n",
        "\n",
        "# histogram using plot\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(dates)\n",
        "plt.show()"
      ],
      "id": "0701834d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efc6d663"
      },
      "source": [
        "# PREPROCESSING"
      ],
      "id": "efc6d663"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8363a00b"
      },
      "source": [
        "# Deleting Special Characters and StopWords"
      ],
      "id": "8363a00b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1594cd8"
      },
      "outputs": [],
      "source": [
        "# Creation of Function for data cleaning\n",
        "def cleanCharacters(tweet):\n",
        "     hash_text = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", '', tweet) #Delete word with hashtag\n",
        "     at_text = re.sub(r'(?:@[\\w_]+)', '', hash_text) #Delete user with at\n",
        "     sp_text = re.sub(r'[^\\w\\s]', '', at_text) #Delete punctuation marks\n",
        "     text_sinlink = re.sub(r'http\\S+', '', text_sp) #Delete Https Links\n",
        "     text_num = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', '',text_sinlink) #Delete numbers\n",
        "     word_tokens = word_tokenize(text_num)\n",
        "\n",
        "     # Delete StopWords(connectors like for,by,of,a,like,etc)\n",
        "     stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "     filtered_sentence = []\n",
        "\n",
        "     for w in word_tokens:\n",
        "         if w not in stop_words:\n",
        "             filtered_sentence.append(w)\n",
        "\n",
        "     return filtered_sentence\n",
        "\n",
        "# Example:\n",
        "# cleanupCharacters('''a #word I use @piero 955584741 üòÇ hi hi papa mama üòÅ news üòÅ (such as \"the\", \"a\", \"a\", \"in\") üòÇüòÅüòÅ from a search engine such as http ://saturdays.ai/2022/04/07/hapyness-analisis-de-sentimientos-en-la-poblacion-de-aragon/''')"
      ],
      "id": "c1594cd8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e89f128"
      },
      "outputs": [],
      "source": [
        "#We use the function \"limpiezaCaracteres\" to obtain lists of the clean words and we use the join to convert it to Object\n",
        "#Everything will be saved in the new column \"tweets_transform\"\n",
        "tweet_eng[\"tweets_transform\"] = tweet_eng[\"tweet\"].apply(limpiezaCaracteres).apply(lambda x:\" \".join(x))"
      ],
      "id": "5e89f128"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3c4a126"
      },
      "outputs": [],
      "source": [
        "# We print the DataFrame with the new column \"Tweets Transform\"\n",
        "tweet esp"
      ],
      "id": "a3c4a126"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "335e0c38"
      },
      "outputs": [],
      "source": [
        "#We save the Dataframe with the tweets without StopWords and Special Characters\n",
        "tweet_eng.to_csv(\"DataTwitterStopWords.csv\", index=False)"
      ],
      "id": "335e0c38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fad9497"
      },
      "source": [
        "## We use only the column with the Transformed Tweets"
      ],
      "id": "2fad9497"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca828b9f"
      },
      "outputs": [],
      "source": [
        "#We eliminate the Tweet column\n",
        "tweet_limpio = tweet_esp.drop(['tweet'], axis=1)"
      ],
      "id": "ca828b9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b810a3c"
      },
      "outputs": [],
      "source": [
        "#We print the new DataFrame\n",
        "tweet_limpio"
      ],
      "id": "5b810a3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34d0fb2a"
      },
      "outputs": [],
      "source": [
        "#We Save Data with Transformed Tweets\n",
        "tweet_limpio.to_csv(\"DataTwitterTransform.csv\", index=False)"
      ],
      "id": "34d0fb2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e8d2cca"
      },
      "source": [
        "# Delete Rows with Special Characters"
      ],
      "id": "0e8d2cca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1eb1183"
      },
      "outputs": [],
      "source": [
        "test = clean_tweet.dropna(axis=0) # first remove nulls\n",
        "with_script = test[test.tweets_transform.str.contains(\"_\")]\n",
        "with_stripe = test[test.tweets_transform.str.contains(\"‰∏Ä\")]\n",
        "with_chinese = test[test.tweets_transform.str.contains(\"ÏÑ∏Í∏∞ ÏÜåÎÖÄ\")]\n",
        "with_may = test[test.tweets_transform.str.contains(\"·îï·ó¥·ó∞·ó©·ëé·ó© ·ó™·ó¥ ·í™·ó© ·ëï·é•·ó¥·ëé·ëï·é•·ó©\")]\n",
        "with_rare = test[test.tweets_transform.str.contains(\"·É¶\")]\n",
        "total = pd.concat([with_dash,with_dash,with_chinese,with_may,with_rare],axis=0) # Joining the dataframes\n",
        "tweet_clean2 = tweet_clean.drop(index=total.index)"
      ],
      "id": "a1eb1183"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95255a3"
      },
      "outputs": [],
      "source": [
        "clean_tweet2"
      ],
      "id": "b95255a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a2ea739"
      },
      "outputs": [],
      "source": [
        "#We Save Data with Transformed Tweets\n",
        "tweet_limpio2.to_csv(\"DataTwitterSinEspecial.csv\", index=False)"
      ],
      "id": "2a2ea739"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298b63cd"
      },
      "source": [
        "# lemmatization"
      ],
      "id": "298b63cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea9db1a3"
      },
      "outputs": [],
      "source": [
        "#We divide the Dataframe into 5 parts\n",
        "list = []\n",
        "for i in range(0.54001,10800):\n",
        "     list.append(clean_tweet2.loc[i:i+10799,:])"
      ],
      "id": "ea9db1a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d99274"
      },
      "outputs": [],
      "source": [
        "data1 = pd.DataFrame(lista[0])\n",
        "data2 = pd.DataFrame(lista[1])\n",
        "data3 = pd.DataFrame(lista[2])\n",
        "data4 = pd.DataFrame(lista[3])\n",
        "data5 = pd.DataFrame(lista[4])"
      ],
      "id": "d1d99274"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73808738"
      },
      "outputs": [],
      "source": [
        "#We keep the 5 divisions to be able to Lematize it by\n",
        "data1.to_csv(\"data1.csv\", index=False)\n",
        "data2.to_csv(\"data2.csv\", index=False)\n",
        "data3.to_csv(\"data3.csv\", index=False)\n",
        "data4.to_csv(\"data4.csv\", index=False)\n",
        "data5.to_csv(\"data5.csv\", index=False)\n"
      ],
      "id": "73808738"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd195b2a"
      },
      "outputs": [],
      "source": [
        "# We print the first 10800 data\n",
        "data1"
      ],
      "id": "cd195b2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1530fb5b"
      },
      "outputs": [],
      "source": [
        "#Create a function to convert tweets to Slogans()\n",
        "def lemmatization(tweet):\n",
        "     nlp = spacy.load('es_core_news_sm')\n",
        "     doc = nlp(tweet)\n",
        "     lemmas = [tok.lemma_.lower() for tok in doc]\n",
        "     object = \" \".join(lemmas)\n",
        "     return object\n",
        "\n",
        "# Lemmatization = Transform from (ask -> ask) or (I am -> be)\n",
        "# # Example\n",
        "#lemmatization('I am a text that is crying out to be processed. hahahahahah')"
      ],
      "id": "1530fb5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e3e5485"
      },
      "outputs": [],
      "source": [
        "#Primeros 10 Tweets sin lematizar\n",
        "# tweet_limpio[\"tweets_transform\"][:10]"
      ],
      "id": "3e3e5485"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3075d79c"
      },
      "outputs": [],
      "source": [
        "##Primeros 10 Tweets lematizados\n",
        "# tweet_limpio[\"tweets_transform\"][:10].apply(lematizacion)"
      ],
      "id": "3075d79c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d08b4c6"
      },
      "source": [
        "### First Lematized Data Division"
      ],
      "id": "9d08b4c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "978dc747"
      },
      "outputs": [],
      "source": [
        "data1[\"tweets_transform\"] = data1[\"tweets_transform\"].astype(str).apply(Lemmatization)"
      ],
      "id": "978dc747"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b0e3ab1"
      },
      "outputs": [],
      "source": [
        "data1"
      ],
      "id": "2b0e3ab1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3fba877"
      },
      "outputs": [],
      "source": [
        "data1.to_csv(\"data1process.csv\", index=False)"
      ],
      "id": "e3fba877"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "702e1fe9"
      },
      "source": [
        "### Second Lematized Data Division"
      ],
      "id": "702e1fe9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c2183a0"
      },
      "outputs": [],
      "source": [
        "data2[\"tweets_transform\"] = data2[\"tweets_transform\"].astype(str).apply(Lemmatization)"
      ],
      "id": "9c2183a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72fd5c7c"
      },
      "outputs": [],
      "source": [
        "data2.to_csv(\"data2process.csv\", index=False)"
      ],
      "id": "72fd5c7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71da55c5"
      },
      "outputs": [],
      "source": [
        "data2"
      ],
      "id": "71da55c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997ce473"
      },
      "source": [
        "### Third Lematized Data Division"
      ],
      "id": "997ce473"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5235402"
      },
      "outputs": [],
      "source": [
        "data3[\"tweets_transform\"] = data3[\"tweets_transform\"].astype(str).apply(Lemmatization)"
      ],
      "id": "e5235402"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e72b8553"
      },
      "outputs": [],
      "source": [
        "data3.to_csv(\"data3process.csv\", index=False)"
      ],
      "id": "e72b8553"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8545f44"
      },
      "outputs": [],
      "source": [
        "data3"
      ],
      "id": "b8545f44"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcfeefd"
      },
      "source": [
        "### Fourth Division of Lematized Data"
      ],
      "id": "ddcfeefd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "604afb6a"
      },
      "outputs": [],
      "source": [
        "data4[\"tweets_transform\"] = data4[\"tweets_transform\"].astype(str).apply(Lemmatization)"
      ],
      "id": "604afb6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f796f4e2"
      },
      "outputs": [],
      "source": [
        "data4.to_csv(\"data4process.csv\", index=False)"
      ],
      "id": "f796f4e2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9e03334"
      },
      "outputs": [],
      "source": [
        "data4"
      ],
      "id": "f9e03334"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5fa43af"
      },
      "source": [
        "### Fifth Lemmatized Data Division"
      ],
      "id": "c5fa43af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a7f1e1d"
      },
      "outputs": [],
      "source": [
        "data5[\"tweets_transform\"] = data5[\"tweets_transform\"].astype(str).apply(Lemmatization)"
      ],
      "id": "6a7f1e1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f95232c"
      },
      "outputs": [],
      "source": [
        "data5"
      ],
      "id": "3f95232c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d5c72c2"
      },
      "outputs": [],
      "source": [
        "data5.to_csv(\"data5process.csv\", index=False)"
      ],
      "id": "7d5c72c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ed5019"
      },
      "source": [
        "## Joining the data division in a DataFrame"
      ],
      "id": "a2ed5019"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "897c492f"
      },
      "outputs": [],
      "source": [
        "d12 = pd.concat([data1,data2]) #Joining Data1 and Data 2\n",
        "d123 = pd.concat([d12,data3]) #Joining Data12 and Data 3\n",
        "d1234 = pd.concat([d123,data4]) #Joining Data123 and Data 4\n",
        "processed_tweet = pd.concat([d1234,data5]) #Joining Data1234 and Data 5"
      ],
      "id": "897c492f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3f192f1"
      },
      "outputs": [],
      "source": [
        "tweet_procesado"
      ],
      "id": "a3f192f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b147bbc"
      },
      "outputs": [],
      "source": [
        "#Saving the Processed Data\n",
        "tweet_procesado.to_csv(\"DataLematizada.csv\", index=False)"
      ],
      "id": "0b147bbc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4122e0a"
      },
      "source": [
        "# An√°lisis de Sentimientos"
      ],
      "id": "b4122e0a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b8fb231"
      },
      "outputs": [],
      "source": [
        "clasificacion = pipeline(\"sentiment-analysis\")"
      ],
      "id": "3b8fb231"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62809acc"
      },
      "outputs": [],
      "source": [
        "#Funcion para el Analisis de Sentimientos\n",
        "def analisis(data):\n",
        "    results = clasificacion(data)\n",
        "    return results\n",
        "\n",
        "#Ejemplos\n",
        "# analisis(\"ser triste\")"
      ],
      "id": "62809acc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b7251a2"
      },
      "outputs": [],
      "source": [
        "results = tweet_procesado[\"tweets_transform\"].apply(analisis)\n",
        "results"
      ],
      "id": "3b7251a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b04f0dad"
      },
      "outputs": [],
      "source": [
        "sentimientos = results.copy()"
      ],
      "id": "b04f0dad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12085e70"
      },
      "outputs": [],
      "source": [
        "ind = 0\n",
        "positivo = []\n",
        "comentario = []\n",
        "scores = []\n",
        "\n",
        "for result in sentimientos:\n",
        "    #print(result[0]['label'])\n",
        "    comentario.append(result[0]['label'])\n",
        "    positivo.append(tweet_procesado['tweets_transform'][ind])\n",
        "    #print({result['label']},{round(result['score'],4)})\n",
        "\n",
        "    scores.append(round(result[0]['score'],4))\n",
        "    ind=ind + 1"
      ],
      "id": "12085e70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8359144c"
      },
      "outputs": [],
      "source": [
        "dataSentimientos = pd.DataFrame({\"Comentario\":comentario,\"Score\":scores,\"Tweet\":positivo})"
      ],
      "id": "8359144c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb60cce5"
      },
      "outputs": [],
      "source": [
        "dataSentimientos"
      ],
      "id": "fb60cce5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8928849"
      },
      "outputs": [],
      "source": [
        "dataSentimientos.to_csv(\"DataTwitterSentimientos.csv\", index=False)"
      ],
      "id": "b8928849"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8197e4d0"
      },
      "outputs": [],
      "source": [
        "def tokenization(text):\n",
        "    text = word_tokenize(text.lower())\n",
        "    return text"
      ],
      "id": "8197e4d0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ac722a"
      },
      "outputs": [],
      "source": [
        "nube = dataSentimientos.copy()\n",
        "nube[\"Lista\"] = nube['Tweet'].apply(tokenization)"
      ],
      "id": "18ac722a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "008979ee"
      },
      "outputs": [],
      "source": [
        "nube"
      ],
      "id": "008979ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73daac53"
      },
      "outputs": [],
      "source": [
        "nube.to_csv(\"DataTwitterNube.csv\", index=False)"
      ],
      "id": "73daac53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44aa36bc"
      },
      "outputs": [],
      "source": [
        "nubePalabras = nube.explode(\"Lista\")\n",
        "nubePalabras"
      ],
      "id": "44aa36bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a51b5b80"
      },
      "outputs": [],
      "source": [
        "nubePalabras.to_csv(\"DataTwitterNubeExplode.csv\", index=False)"
      ],
      "id": "a51b5b80"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43976500"
      },
      "outputs": [],
      "source": [
        "words = list(itertools.chain(nubePalabras['Lista']))\n",
        "wf = collections.Counter(words)\n",
        "wf.most_common(20)"
      ],
      "id": "43976500"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f498f83c"
      },
      "outputs": [],
      "source": [
        "words = ' '.join(nube['Tweet'])"
      ],
      "id": "f498f83c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f369ad3"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(background_color=\"white\",\n",
        "                      # stopwords = stopwords,\n",
        "                    #   colormap = \"icefire\",\n",
        "                      scale = 2).generate(words)\n",
        "# Display the generated image:\n",
        "plt.figure(figsize = (15, 15), dpi = 300, facecolor = None)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)"
      ],
      "id": "7f369ad3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ddf851c"
      },
      "outputs": [],
      "source": [
        "a = nubePalabras['Lista'].value_counts().sort_values(ascending = True).tail(16)\n",
        "x = a.index\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.style.use('seaborn-white')\n",
        "plt.barh(x,a, color = \"firebrick\")\n",
        "plt.xlabel(\"Counts\", fontsize = 30)\n",
        "plt.xticks(fontsize = 20)\n",
        "plt.yticks(fontsize = 20)"
      ],
      "id": "7ddf851c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be67aa91"
      },
      "outputs": [],
      "source": [
        "dataSentimientos[\"Comentario\"].value_counts()"
      ],
      "id": "be67aa91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e5e336f"
      },
      "outputs": [],
      "source": [
        "dataSentimientos[\"target\"]=dataSentimientos[\"Comentario\"].map({'NEGATIVE': 0, 'POSITIVE':1})"
      ],
      "id": "1e5e336f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "824ef713"
      },
      "outputs": [],
      "source": [
        "dataSentimientos"
      ],
      "id": "824ef713"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "698b5cf6"
      },
      "outputs": [],
      "source": [
        "dataSentimientos.to_csv(\"DataTwitterSentimientoTarget.csv\", index=False)"
      ],
      "id": "698b5cf6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20129740"
      },
      "outputs": [],
      "source": [
        "dataSentimientos[\"target\"].value_counts()"
      ],
      "id": "20129740"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0920008"
      },
      "outputs": [],
      "source": [
        "plt.hist(dataSentimientos['target'],bins=4)"
      ],
      "id": "b0920008"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecaf488c"
      },
      "source": [
        "# Modelos de Machine Learning"
      ],
      "id": "ecaf488c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7befddd2"
      },
      "source": [
        "## Division de Datos"
      ],
      "id": "7befddd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc8e1a87"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from xgboost import XGBClassifier"
      ],
      "id": "bc8e1a87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb8798c6"
      },
      "outputs": [],
      "source": [
        "datos = dataSentimientos.sample(n=10000)\n",
        "datos.reset_index(inplace=True, drop=True)\n",
        "datos"
      ],
      "id": "eb8798c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad4adc71"
      },
      "outputs": [],
      "source": [
        "X = datos['Tweet']\n",
        "X\n"
      ],
      "id": "ad4adc71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e2d5eb1"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "tfidf_wm = tfidfvectorizer.fit_transform(X)\n",
        "tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
        "\n",
        "ml = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
        "contarVec = pd.concat((datos[[\"Comentario\", 'Score',\"target\"]], ml), axis = 1)\n",
        "contarVec"
      ],
      "id": "1e2d5eb1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f9a022c"
      },
      "outputs": [],
      "source": [
        "contarVec.to_csv(\"DataTwitterCountVectorizer.csv\", index=False)"
      ],
      "id": "9f9a022c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b87c139"
      },
      "outputs": [],
      "source": [
        "contarVec2 = contarVec.copy()\n",
        "datos_X = contarVec2.drop([\"Comentario\",\"Score\",\"target\"], axis = 1)\n",
        "datos_y = contarVec2[\"target\"]"
      ],
      "id": "3b87c139"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f763069"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(datos_X,datos_y,test_size = 0.2,random_state = 123)"
      ],
      "id": "6f763069"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d679bea0"
      },
      "outputs": [],
      "source": [
        "print(\"Total de datos en X Entrenamiento: \",X_train.shape[0])\n",
        "print(\"Total de datos en X Test: \",X_test.shape[0])\n",
        "print(\"Total de datos en Y Entrenamiento: \",y_train.shape[0])\n",
        "print(\"Total de datos en Y Test: \",y_test.shape[0])"
      ],
      "id": "d679bea0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bb93dda"
      },
      "source": [
        "## Naive Bayes"
      ],
      "id": "9bb93dda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f701b67"
      },
      "source": [
        "### Gaussian Naive Bayes"
      ],
      "id": "0f701b67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c758bcf"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "modelGNB = GaussianNB()\n",
        "modelGNB.fit(X_train, y_train)\n",
        "modelGNB"
      ],
      "id": "5c758bcf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0feaa41f"
      },
      "outputs": [],
      "source": [
        "prediction_gnb = modelGNB.predict(X_test)\n",
        "conf_gnb       = confusion_matrix(y_test, prediction_gnb)\n",
        "acc_gnb        = accuracy_score(y_test, prediction_gnb)\n",
        "prec_gnb       = precision_score(y_test, prediction_gnb, average=\"weighted\")\n",
        "rec_gnb        = recall_score(y_test, prediction_gnb, average=\"weighted\")\n",
        "f1_gnb         = f1_score(y_test, prediction_gnb, average=\"weighted\")\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", conf_gnb, '\\n')\n",
        "print(\"Accuracy    : \", acc_gnb)\n",
        "print(\"Recall      : \", prec_gnb)\n",
        "print(\"Precision   : \", rec_gnb)\n",
        "print(\"F1 Score    : \", f1_gnb)"
      ],
      "id": "0feaa41f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41cb7c2e"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,prediction_gnb))"
      ],
      "id": "41cb7c2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7b83d61"
      },
      "source": [
        "### Multinomial Naive Bayes"
      ],
      "id": "f7b83d61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "748d95a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "modelMNB = MultinomialNB()\n",
        "modelMNB.fit(X_train, y_train)\n",
        "modelMNB"
      ],
      "id": "748d95a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c31fc094"
      },
      "outputs": [],
      "source": [
        "prediction_MNB = modelMNB.predict(X_test)\n",
        "conf_mnb      = confusion_matrix(y_test, prediction_MNB)\n",
        "acc_mnb       = accuracy_score(y_test, prediction_MNB)\n",
        "prec_mnb       = precision_score(y_test, prediction_MNB, average=\"weighted\")\n",
        "rec_mnb        = recall_score(y_test, prediction_MNB, average=\"weighted\")\n",
        "f1_mnb        = f1_score(y_test, prediction_MNB, average=\"weighted\")\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", conf_mnb, '\\n')\n",
        "print(\"Accuracy    : \", acc_mnb)\n",
        "print(\"Recall      : \", prec_mnb)\n",
        "print(\"Precision   : \", rec_mnb)\n",
        "print(\"F1 Score    : \", f1_mnb)"
      ],
      "id": "c31fc094"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30c58429"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,prediction_MNB))"
      ],
      "id": "30c58429"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30647c33"
      },
      "source": [
        "## Random Forest Clasifier"
      ],
      "id": "30647c33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46943057"
      },
      "outputs": [],
      "source": [
        "modelRF = RandomForestClassifier(random_state = 123) #valores por defecto\n",
        "modelRF = modelRF.fit(X_train, y_train)\n",
        "modelRF"
      ],
      "id": "46943057"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7afeef02"
      },
      "outputs": [],
      "source": [
        "#EVALUATION\n",
        "prediction_rf = modelRF.predict(X_test)\n",
        "conf_rf       = confusion_matrix(y_test, prediction_rf)\n",
        "acc_rf        = accuracy_score(y_test, prediction_rf)\n",
        "prec_rf       = precision_score(y_test, prediction_rf, average=\"weighted\")\n",
        "rec_rf        = recall_score(y_test, prediction_rf, average=\"weighted\")\n",
        "f1_rf         = f1_score(y_test, prediction_rf, average=\"weighted\")\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", conf_rf, '\\n')\n",
        "print(\"Accuracy    : \", acc_rf)\n",
        "print(\"Recall      : \", prec_rf)\n",
        "print(\"Precision   : \", rec_rf)\n",
        "print(\"F1 Score    : \", f1_rf)"
      ],
      "id": "7afeef02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99bd6384"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,prediction_rf))"
      ],
      "id": "99bd6384"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fb559b4"
      },
      "source": [
        "## SVM"
      ],
      "id": "6fb559b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "839b63e2"
      },
      "outputs": [],
      "source": [
        "modelSVM = svm.SVC(random_state = 1) #valores por defecto\n",
        "modelSVM = modelSVM.fit(X_train, y_train)\n",
        "modelSVM"
      ],
      "id": "839b63e2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11a56e34"
      },
      "outputs": [],
      "source": [
        "prediction_svm = modelSVM.predict(X_test)\n",
        "conf_svm       = confusion_matrix(y_test, prediction_svm)\n",
        "acc_svm        = accuracy_score(y_test, prediction_svm)\n",
        "prec_svm       = precision_score(y_test, prediction_svm, average=\"weighted\")\n",
        "rec_svm        = recall_score(y_test, prediction_svm, average=\"weighted\")\n",
        "f1_svm         = f1_score(y_test, prediction_svm, average=\"weighted\")\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", conf_svm, '\\n')\n",
        "print(\"Accuracy    : \", acc_svm)\n",
        "print(\"Recall      : \", prec_svm)\n",
        "print(\"Precision   : \", rec_svm)\n",
        "print(\"F1 Score    : \", f1_svm)"
      ],
      "id": "11a56e34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a27b5ee7"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,prediction_svm))"
      ],
      "id": "a27b5ee7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fd39b7"
      },
      "source": [
        "## Comparacion de Modelos"
      ],
      "id": "50fd39b7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f87c4c2"
      },
      "outputs": [],
      "source": [
        "gnb = pd.DataFrame({\"Parametros\":[\"Accuracy\", \"Recall\", \"Precision\", \"F1_score\"],\n",
        "                    \"Valor\":[acc_gnb, prec_gnb, rec_gnb, f1_gnb]})\n",
        "gnb[\"Modelo\"] = \"GNB\"\n",
        "\n",
        "mnb = pd.DataFrame({\"Parametros\":[\"Accuracy\", \"Recall\", \"Precision\", \"F1_score\"],\n",
        "                    \"Valor\":[acc_mnb, prec_mnb, rec_mnb, f1_mnb]})\n",
        "mnb[\"Modelo\"] = \"MNB\"\n",
        "\n",
        "rf = pd.DataFrame({\"Parametros\":[\"Accuracy\", \"Recall\", \"Precision\", \"F1_score\"],\n",
        "                    \"Valor\":[acc_rf, prec_rf, rec_rf, f1_rf]})\n",
        "rf[\"Modelo\"] = \"RF\"\n",
        "svm = pd.DataFrame({\"Parametros\":[\"Accuracy\", \"Recall\", \"Precision\", \"F1_score\"],\n",
        "                    \"Valor\":[acc_svm, prec_svm, rec_svm, f1_svm]})\n",
        "svm[\"Modelo\"] = \"SVM\"\n",
        "comparacion = pd.concat((gnb, mnb, rf, svm), axis = 0).reset_index(drop = True)\n",
        "comparacion[\"Valorador\"] = \"Grupo1\""
      ],
      "id": "1f87c4c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87b53824"
      },
      "outputs": [],
      "source": [
        "sns.set_context(\"notebook\", font_scale = 1.7)\n",
        "# sns.set_theme(style=\"ticks\")\n",
        "sns.set_palette(\"pastel\")\n",
        "a = sns.barplot(data = comparacion, x = \"Parametros\", y = \"Valor\", hue = \"Modelo\")\n",
        "# sns.move_legend(a, \"lower center\", bbox_to_anchor=(1.2, 0.6), title=\"Model\", frameon=False)\n",
        "plt.legend(bbox_to_anchor=(1.1, 1), loc='upper left', borderaxespad=0)"
      ],
      "id": "87b53824"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "5d3779f81df8335e237640b66ede365d4b45a9f224258c660b8326a9b4050db4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}